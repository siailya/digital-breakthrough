{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from gensim import utils\n",
    "from gensim.models import FastText\n",
    "from razdel import tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/work/hack/datasets/dataset_1.csv\")\n",
    "\n",
    "for i in range(1, 5):\n",
    "    new_data = pd.read_csv(f\"/work/hack/datasets/dataset_{i}.csv\")\n",
    "    data = pd.concat([data, new_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-4d1fa193e911>:1: DtypeWarning: Columns (4,6,7,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  building = pd.read_csv(\"/work/hack/additional_data/building_20230808.csv\")\n"
     ]
    }
   ],
   "source": [
    "building = pd.read_csv(\"/work/hack/additional_data/building_20230808.csv\")\n",
    "building = building[[\"id\", \"short_address\"]]\n",
    "building_1 = building.copy().sample(frac=1).reset_index(drop=True)\n",
    "building_1 = building_1.rename(columns={\"id\": \"id2\", \"short_address\": \"short_address2\"})\n",
    "building_zero = pd.concat([building, building_1], axis=1)\n",
    "building_one = pd.concat([building, building.rename(columns={\"id\": \"id2\", \"short_address\": \"short_address2\"})], axis=1)\n",
    "building_new = pd.concat([building_zero, building_one])\n",
    "building_new = building_new.rename(columns={\n",
    "    \"id\": \"first_id\",\n",
    "    \"short_address\": \"first_text\",\n",
    "    \"id2\": \"second_id\",\n",
    "    \"short_address2\": \"second_text\"\n",
    "})\n",
    "building_new = building_new.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text_model = FastText.load(\"/work/hack/fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb(text: str):\n",
    "    tokens = list(tokenize(text))\n",
    "    tokens = [_.text for _ in tokens]\n",
    "    predict = np.array([fast_text_model.wv[token] for token in tokens])\n",
    "    predict = np.mean(predict, axis=0)\n",
    "    predict = predict / np.linalg.norm(predict)\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, texts: pd.DataFrame):\n",
    "        self.first_emb = np.array(texts['first_text'].apply(emb).values.tolist())\n",
    "        self.second_emb = np.array(texts['second_text'].apply(emb).values.tolist())\n",
    "        self.labels = (texts[\"first_id\"] == texts[\"second_id\"]).astype(np.int8)\n",
    "        \n",
    "    def __getitem__(self, ind: int):\n",
    "        concat_embs = np.concatenate([self.first_emb[ind], self.second_emb[ind]])\n",
    "        label = torch.zeros(2)\n",
    "        label[self.labels.iloc[ind]] = 1\n",
    "        return torch.tensor(concat_embs), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.first_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, inp_shape: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(inp_shape, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.fc3 = nn.Linear(100, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(building_new[:10000])\n",
    "val_dataset = MyDataset(building_new[10000:11000])\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
    "val_data_loader = DataLoader(dataset=val_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 0.685\n",
      "accuracy: 0.912\n",
      "[2,    50] loss: 0.223\n",
      "accuracy: 0.956\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_data_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    \n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 50:.3f}')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    net.eval()\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_data_loader:\n",
    "            inputs, labels = data\n",
    "            outputs = net(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    cnt += 1\n",
    "    print(f\"accuracy: {cnt / 1000}\")\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
